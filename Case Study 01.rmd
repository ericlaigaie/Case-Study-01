---
title: "Case Study - 01"
author: "Eric Laigaie"
date: "10/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(tidyverse)
library(readr)
library(ggplot2)
library(choroplethr)
library(choroplethrMaps)
library(ggpubr)
library(class)
library(caret)
library(e1071)
library(multcomp)
library(DescTools)
```

```{r}
# Load in Both Beers and Breweries datasets.
beers <- read_csv(url("https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/Beers.csv?token=AL7L5SUDXIN5C6OJ62LPMELBLTXMC"))
breweries <- read_csv(url("https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/Breweries.csv?token=AL7L5SSPZ7B6NQBXJIMILGDBLTXOM"))
```

1.   How many breweries are present in each state?
```{r State n, fig.height=5}
# Brewery Bar Chart
breweries %>%
  group_by(State) %>%
  mutate(n = n()) %>%
  ggplot(aes(x=n, y=reorder(State, n))) + 
  geom_bar(stat='identity', fill='darkseagreen4') + 
  labs(x='State', y='Number of Breweries', title='Number of Breweries by State') + 
  theme_minimal()

# Load in State Brewery Counts (This changes State name from "AL" -> "alabama")
states <- read_csv(url("https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/state_brewery_counts.csv"))

# Import geojson for state polygons
data(continental_us_states)

# Designate region & value
states$region <- tolower(states$State)
states$value <- states$n

# Create Choropleth
state_choropleth(states, 
                 num_colors=9,
                 zoom = continental_us_states) +
  scale_fill_brewer(palette="Greens") +
  labs(title = "Breweries by State",
       fill = "n") +
  guides(fill=guide_legend(title="N"))
```

2.   Merge beer data with the breweries data. Print the first 6 observations and the last six observations to check the merged file.  (RMD only, this does not need to be included in the presentation or the deck.)
```{r}
# Merge beers and breweries
breweries <- breweries %>% rename(Brewery_id=Brew_ID)
df <- merge(breweries, beers, by='Brewery_id')

df <- df %>% rename(Brewery = Name.x, Beer = Name.y)

# Print top & bottom 6
head(df, 6)
tail(df, 6)
```

3.   Address the missing values in each column.
```{r}
# METHOD 1 - REMOVE ALL

# Print all columns and # of NA's
#sapply(df, function(x) sum(is.na(x)))

# Remove all NA's
#df <- na.omit(df)
```

```{r}
# METHOD 2 - EQUALS ZERO

# Set NA ABV's and IBU's to 0
df$ABV[is.na(df$ABV)] = 0
df$IBU[is.na(df$IBU)] = 0

# Show that remaining NA's are in 'Style'
sum(is.na(df))
sum(is.na(df$Style))

# Remove 5 missing 'Styles'
df = na.omit(df)
```

```{r}
# METHOD 3 - MEAN IMPUTATION BY STYLE GROUP

# Remove missing Styles
df <- df %>% filter(is.na(Style) == FALSE)

# Function takes in df and imputes mean ABV and IBU. Also creates Style_Group column
imputer <- function(mydf, myText) {
  temp <- {{mydf}}
  for(i in 7:8){
    temp[is.na(temp[,i]), i] <- mean(temp[,i], na.rm = TRUE)
  }
  temp$Style_Group <- {{myText}}
  output <- temp
}

# Creating new dfs for each style group
ale <- df %>% filter(grepl('Ale', df$Style) == TRUE) %>% filter(Style != 'English India Pale Ale (IPA)')
ipa <- df %>% filter(grepl('IPA', df$Style) == TRUE)
lager <- df %>% filter(grepl('Lager', df$Style) == TRUE)
stout <- df %>% filter(grepl('Stout', df$Style) == TRUE)
other <- df %>% filter(grepl('Stout', df$Style) == FALSE &
                grepl('Lager', df$Style) == FALSE &
                grepl('IPA', df$Style) == FALSE &
                grepl('IPA', df$Style) == FALSE &
                grepl('Ale', df$Style) == FALSE)

# Imputing mean ABV / IBU
ale <- imputer(ale, 'Ale')
ipa <- imputer(ipa, 'IPA')
lager <- imputer(lager, 'Lager')
stout <- imputer(stout, 'Stout')
other <- imputer(other, 'Other')

# Combine back into df
df <- rbind(ale, ipa, lager, stout, other)
df$Style_Group <- as.factor(df$Style_Group)
```

4.   Compute the median alcohol content and international bitterness unit for each state. Plot a bar chart to compare.
```{r med_abv, fig.height=5}
# Median ABV bar chart
df %>% group_by(State) %>% 
  summarise(med_abv = median(ABV)) %>%
  ggplot(aes(y=reorder(State, med_abv), x=med_abv)) + 
  geom_bar(stat='identity', fill='darkseagreen4') +
  labs(x='Median ABV', y='State', title='Median ABV by State')

# Median IBU bar chart
df %>% group_by(State) %>%
  summarise(med_ibu = median(IBU)) %>%
  ggplot(aes(y=reorder(State, med_ibu), x=med_ibu)) + 
  geom_bar(stat='identity', fill='darkseagreen4') +
  labs(x='Median IBU', y='State', title='Median IBU by State')
```

5.   Which state has the maximum alcoholic (ABV) beer? Which state has the most bitter (IBU) beer?
```{r}
# Find max ABV & IBU
max_ABV = max(df$ABV)
max_IBU = max(df$IBU)
df %>% filter(ABV == max_ABV)
df %>% filter(IBU == max_IBU)
```

6.   Comment on the summary statistics and distribution of the ABV variable.
```{r}
# Quick summary of ABV - full dataset
summary(df$ABV)
sd(df$ABV)

# Make 4 charts
box_all <- ggplot(df, aes(x=ABV)) + geom_boxplot(binwidth=.01, fill='darkseagreen4') + 
  labs(x='ABV', title='ABV Boxplot')
hist_all <- ggplot(df, aes(x=ABV)) + geom_histogram(color='black', fill='darkseagreen4') +
  labs(x='ABV', title='ABV Histogram')

# Arrange 4 charts
ggarrange(box_all, hist_all, nrow=2, ncol=1)

# Check for normality
shapiro.test(df$ABV)
qqnorm(df$ABV, col='darkseagreen4', lwd=2)
```

7.   Is there an apparent relationship between the bitterness of the beer and its alcoholic content? Draw a scatter plot.  Make your best judgment of a relationship and EXPLAIN your answer.
```{r}
# Find correlation of IBU and ABV in df
cor_IBUABV = round(cor(df$IBU, df$ABV), 3)

# Make R label
label_x = paste(c("R =", cor_IBUABV), collapse = " ")

# IBU and ABV scatterplot (with R label)
ggplot(df, aes(x=IBU, ABV)) + 
  geom_point(size=2, color='darkseagreen3') + 
  geom_smooth(method='lm', color='darkseagreen4') +
  labs(x='International Bittering Units (IBU)', y='Alcohol by Volume (ABV)', title='IBU vs ABV') +
  geom_label(label=label_x, x=100, y=.025,label.padding = unit(0.55, "lines"), label.size = 0.35, color = "black", fill="#69b3a2")

# BELOW ONLY APPLICABLE WHEN NA METHOD 2 IS USED

# Make new df, df_alc, which contains rows with IBU > 0 and ABV > 0
#df_alc <- df %>%
#  filter(IBU > 0 & ABV > 0)

# Find correlation of IBU and ABV in df_alc
#cor_alcIBUABV = round(cor(df_alc$IBU, df_alc$ABV), 3)

# Make R label
#label_y = paste(c("R = ", cor_alcIBUABV), collapse = " ")

# IBU and ABV scatterplot (From df_alc, with R label)
#ggplot(df_alc, aes(y=ABV,x=IBU)) + 
#  geom_point(size=2, color='darkseagreen3') + 
#  geom_smooth(method='lm',color='darkseagreen4') + 
#  labs(x='International Bittering Units (IBU)', y='Alcohol by Volume (ABV)', title='IBU vs ABV (IBU & ABV > 0)') +
#  geom_label(label=label_y, x=100, y=0.035,label.padding = unit(0.55, "lines"), label.size = 0.35, color = "black", #fill="#69b3a2")
```

8.  Budweiser would also like to investigate the difference with respect to IBU and ABV between IPAs (India Pale Ales) and other types of Ale (any beer with “Ale” in its name other than IPA).  You decide to use KNN classification to investigate this relationship.  Provide statistical evidence one way or the other. You can of course assume your audience is comfortable with percentages … KNN is very easy to understand conceptually.
```{r}

# Create new data frame with only Ale and IPA Beers
# Next, create dummy variable for Ale vs IPA

df_ale = df %>% filter(grepl("Ale", df$Style) == TRUE) %>% filter(Style != 'English India Pale Ale (IPA)')
df_IPA = df %>% filter(grepl("IPA", df$Style) == TRUE)

df_IPA$ale_IPA = 'IPA'
df_ale$ale_IPA = 'Ale'

new_df = rbind(df_ale,df_IPA)

new_df$ale_IPA = as.factor(new_df$ale_IPA)

new_df <- df %>% filter(Style_Group %in% c('Ale', 'IPA'))

# Perform KNN model using ABV and IBU
# Hyper-tune to be completed on K value

# First, standardize IBU and ABV values

new_df$zABV = as.numeric(scale(new_df$ABV))
new_df$zIBU = as.numeric(scale(new_df$IBU))

# Create 70/30 Train/Test Split
len <- length(new_df$Style_Group)
trainInd = sample(seq(1,len,1), .7*len)
train = new_df[trainInd,]
test = new_df[-trainInd,]

# Initial run of KNN model with K=5 
classifications = knn(train[,c(7,8)],test[,c(7,8)],train$Style_Group,prob = TRUE, k = 5)
confusionMatrix(table(classifications,test$Style_Group))

# Loop to find best value for K with 70/30 train-test split

iterations = 100
numks = 100
splitPerc = .70

masterAcc = matrix(nrow = iterations, ncol = numks)
masterSen = matrix(nrow = iterations, ncol = numks)
masterSpe = matrix(nrow = iterations, ncol = numks)

for(j in 1:iterations)
{
  trainIndices = sample(1:dim(new_df)[1],round(splitPerc * dim(new_df)[1]))
  train = new_df[trainIndices,]
  test = new_df[-trainIndices,]
  for(i in 1:numks)
  {
    classifications = knn(train[,c(7,8)],test[,c(7,8)],train$Style_Group, prob = TRUE, k = i)
    table(classifications,test$Style_Group)
    CM = confusionMatrix(table(classifications,test$Style_Group))
    masterAcc[j,i] = CM$overall[1]
    masterSen[j,i] = CM$byClass[1]
    masterSpe[j,i] = CM$byClass[2]
  }
  
}

MeanAcc = colMeans(masterAcc)
MeanSen = colMeans(masterSen)
MeanSpe = colMeans(masterSpe)

plot(seq(1,numks,1),MeanAcc, type = "l")
which.max(MeanAcc)
max(MeanAcc)

plot(seq(1,numks,1),MeanSen, type = "l")
which.max(MeanSen)
max(MeanSen)

plot(seq(1,numks,1),MeanSpe, type = "l")
which.max(MeanSpe)
max(MeanSpe)

# K = 6 gives highest accuracy (81%)
# K = 10 gives highest sensitivity (87%)
# K = 62 gives highest specificity (73%)

# Run KNN model with K=6 for highest accuracy (per hyper tuning)
classifications = knn(train[,c(7,8)],test[,c(7,8)],train$Style_Group,prob = TRUE, k = 5)
confusionMatrix(table(classifications,test$Style_Group))

```

In addition, while you have decided to use KNN to investigate this relationship (KNN is required) you may also feel free to supplement your response to this question with any other methods or techniques you have learned.  Creativity and alternative solutions are always encouraged.
```{r}
# Using Naive-Bayes to investigate relationship

# Set seed and create Train/Test Split 
set.seed(10)
trainIndices = sample(seq(1:length(new_df$Beer_ID)),round(.7*length(new_df$Beer_ID)))
train_nb = new_df[trainIndices,]
test_nb = new_df[-trainIndices,]

# Create model and print confusion matrix
model = naiveBayes(train_nb[,c(7,8)],train_nb$Style_Group,laplace = 1)
table(predict(model,test_nb[,c(7,8)]),test_nb$Style_Group)
CM = confusionMatrix(table(predict(model,test_nb[,c(7,8)]),test_nb$Style_Group))
CM


```

9. Knock their socks off!  Find one other useful inference from the data that you feel Budweiser may be able to find value in.  You must convince them why it is important and back up your conviction with appropriate statistical evidence.
```{r}
new <- df %>%
  group_by(State) %>%
  summarize(
    mean_IBU = mean(IBU), mean_ABV = mean(ABV)
  )

states$mean_IBU <- new$mean_IBU
states$mean_ABV <- new$mean_ABV

# Import geojson for state polygons
data(continental_us_states)

# Designate region & value
choro_df <- data.frame('region' = states$State, 'value' = states$mean_IBU)

# Create Choropleth
state_choropleth(choro_df, 
                 num_colors=6,
                 zoom = continental_us_states) +
  scale_fill_brewer(palette="Greens") +
  labs(title = "Average IBU by State",
       fill = "n") +
  guides(fill=guide_legend(title="Mean IBU"))

# Designate region & value
choro_df <- data.frame('region' = states$State, 'value' = states$mean_ABV)

# Create Choropleth
state_choropleth(choro_df, 
                 num_colors=6,
                 zoom = continental_us_states) +
  scale_fill_brewer(palette="Greens") +
  labs(title = "Average ABV by State",
       fill = "n") +
  guides(fill=guide_legend(title="Mean ABV"))

choro_plotter <- function(myStyle) {
  temp <- df %>% group_by(State, Style_Group) %>% summarize(n = n())
  temp <- temp %>% group_by(State) %>% mutate(Percent = 100*n/sum(n))
  temp <- temp %>% filter(Style_Group == {{myStyle}})
  
  print(temp)
  
  choro_df <- data.frame('region' = states$State, 'value' = temp$Percent)
  
  state_choropleth(choro_df,
                   num_colors=6,
                   zoom = continental_us_states) +
    scale_fill_brewer(palette='Greens') +
    labs(title=paste('Percent of', {{myStyle}}, 'beers by State', sep=" ",collapse=NULL), fill='n') +
    guides(fill=guide_legend(title=paste({{myStyle}}, 'percent', sep = " ", collapse = NULL)))
}

choro_plotter('Ale')
choro_plotter('IPA')
choro_plotter('Stout')
choro_plotter('Lager')
choro_plotter('Other')

regions = read_csv(url('https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/regions_v2.csv'))

# remove State Column and rename abbreviations to 'State' to merge
regions = subset(regions, select = -c(State))
names(regions)[2] = 'State'


new_df2 = merge(df,regions,"State")
ggplot(new_df2, aes(x=Region, fill=Region)) + geom_bar()

temp<-new_df2 %>% group_by(Region) %>% summarize(meanABV = mean(ABV), meanIBU =mean(IBU))
ggplot(temp, aes(x=meanABV, y=meanIBU, color=Region)) +geom_point() +geom_text(aes(label=Region))

fill <- ggplot(new_df2, aes(y=Region, fill=Style_Group)) + geom_bar(position='fill', color='black') + labs(y='Region', x='Proportions of Beers (Filled by Style)', title='Style Proportions by Region')

stack <- ggplot(new_df2, aes(y=Region, fill=Style_Group)) + geom_bar(position='stack', color='black') + labs(y='Region', x='Count of Beers (Filled by Style)', title='Style Counts by Region')

ggarrange(stack, fill, nrow=2)




fit <- aov(ABV ~ Region, data=new_df2)
summary(fit)

fit<-kruskal.test(IBU ~ Region, data=new_df2)
kruskal.test(ABV ~ Region, data=new_df2)

PostHocTest(fit, method = "bonferroni")

quantile(new_df2$ABV, c(.3,.7))
quantile(new_df2$IBU, c(.3,.7))

new_df2$ABV_tier <- cut(new_df2$ABV, breaks = c(0, .051, .065, 1), labels = c("Low","Medium","High"))
new_df2$IBU_tier <- cut(new_df2$IBU, breaks = c(0, 26.55937, 45.03922, 200), labels = c("Low","Medium","High"))

fill2 <- ggplot(new_df2, aes(y=Region, fill=ABV_tier)) + geom_bar(position='fill', color='black') + labs(y='Region', x='Proportions of Beers (Filled by ABV_tier)', title='ABV_tier Proportions by Region')

stack2 <- ggplot(new_df2, aes(y=Region, fill=ABV_tier)) + geom_bar(position='stack', color='black') + labs(y='Region', x='Count of Beers (Filled by ABV_tier)', title='ABV_tier Counts by Region')

fill3 <- ggplot(new_df2, aes(y=Region, fill=IBU_tier)) + geom_bar(position='fill', color='black') + labs(y='Region', x='Proportions of Beers (Filled by IBU_tier)', title='IBU_tier Proportions by Region')

stack3 <- ggplot(new_df2, aes(y=Region, fill=IBU_tier)) + geom_bar(position='stack', color='black') + labs(y='Region', x='Count of Beers (Filled by IBU_tier)', title='IBU_tier Counts by Region')

ggarrange(stack2, fill2, nrow=2)
ggarrange(stack3, fill3, nrow=2)


ggplot(new_df2, aes(x=ABV, y=Region,fill=Region)) + geom_boxplot() + facet_wrap(~Style_Group) + theme(legend.position='none')

ggplot(new_df2, aes(x=IBU, y=Region,fill=Region)) + geom_boxplot() + facet_wrap(~Style_Group) + theme(legend.position='none')


# Create a word bank with average ABV & IBU per word?

# Create word_bank
word_bank <- c()

# Fill bank with all words used within new_df2$Beer
for (i in 1:length(new_df2$Beer)) {
  current <- new_df2$Beer[i]
  current <- str_split(current, ' ')
  for (curr in current) {
    word_bank <- append(word_bank, curr)
  }
}
# Filter out any duplicates
word_bank <- unique(word_bank)

# Create new vectors for the dataframe below
mean_ABV <- numeric(length(word_bank))
mean_IBU <- numeric(length(word_bank))
count <- numeric(length(word_bank))

# Create dataframe to be filled
word_df <- data.frame("Word" = word_bank, "mean_ABV" = mean_ABV, "mean_IBU" = mean_IBU, 'Count' = count)

# Filter out any words with non-A to Z characters (was causing an error)
letters_only <- function(x) !grepl("[^A-Za-z]", x)
word_df <- word_df %>% filter( letters_only(Word) == TRUE)

# Fill dataframe by searching df and taking mean ABV and IBU for each word in bank
for (i in 1:length(word_df$Word)) {
  current <- word_df$Word[i]
  filtered_new_df2 <- df %>% filter(grepl(current, df$Beer) == TRUE)
  word_df$mean_ABV[i] <- mean(filtered_new_df2$ABV)
  word_df$mean_IBU[i] <- mean(filtered_new_df2$IBU)
  word_df$Count[i] <- length(filtered_new_df2$IBU)
}
# Only words that are common (does include the 'a', 'b' stopwords. Can be removed, don't need to be)
common_words <- word_df %>% filter(Count > 10)

library(ggrepel)

# These are the words with the highest/lowest mean ABV or IBU
label <- c('Imperial', 'Series', 'Double', 'Barrel', 'Hill', 'Nonstop', 'Session', 'Hef', 'Light', 'Wit', 'IPA', 'PA', 'India', 'Wit', 'Beach', 'Hell', 'Weiss')

# Plot with selective labels
ggplot(common_words, aes(x=mean_ABV, y=mean_IBU)) + 
  geom_point(aes(size=Count, color=Count)) + 
  geom_label_repel(data= common_words %>% filter(Word %in% label), aes(label=Word)) +
  scale_colour_gradient(low='darkgreen',high='lightgreen') +
  labs(x = 'Average ABV', y = 'Average IBU', title = 'Average ABV & IBU by Word : Size & Color by Usage Count') +
  theme(legend.position='none')
```

```{r}
# Create word_bank
word_bank_brew <- c()

# Fill bank with all words used within new_df2$Beer
for (i in 1:length(new_df2$Brewery)) {
  current <- new_df2$Brewery[i]
  current <- str_split(current, ' ')
  for (curr in current) {
    word_bank_brew <- append(word_bank_brew, curr)
  }
}
# Filter out any duplicates
word_bank_brew <- unique(word_bank_brew)

# Create new vectors for the dataframe below
mean_ABV <- numeric(length(word_bank_brew))
mean_IBU <- numeric(length(word_bank_brew))
count <- numeric(length(word_bank_brew))

# Create dataframe to be filled
word_df_brew <- data.frame("Word" = word_bank_brew, "mean_ABV" = mean_ABV, "mean_IBU" = mean_IBU, 'Count' = count)

# Filter out any words with non-A to Z characters (was causing an error)
letters_only <- function(x) !grepl("[^A-Za-z]", x)
word_df_brew <- word_df_brew %>% filter( letters_only(Word) == TRUE)

# Fill dataframe by searching df and taking mean ABV and IBU for each word in bank
for (i in 1:length(word_df_brew$Word)) {
  current <- word_df_brew$Word[i]
  filtered_new_df2 <- df %>% filter(grepl(current, df$Brewery) == TRUE)
  word_df_brew$mean_ABV[i] <- mean(filtered_new_df2$ABV)
  word_df_brew$mean_IBU[i] <- mean(filtered_new_df2$IBU)
  word_df_brew$Count[i] <- length(filtered_new_df2$IBU)
}
# Only words that are common (does include the 'a', 'b' stopwords. Can be removed, don't need to be)
common_words_brew <- word_df_brew %>% filter(Count > 10)

# These are the words with the highest/lowest mean ABV or IBU
label <- c('Upslope', 'Oskar', 'Blues', 'Amendment', 'Vivant', 'Hopworks', 'Urban', 'Stevens', 'Matt', 'Capital', 'Surly', 'Mike', 'Caldera', 'Minhas', 'Anderson', 'Point', 'Cider')

# Plot with selective labels
ggplot(common_words_brew, aes(x=mean_ABV, y=mean_IBU)) + 
  geom_point(aes(size=Count, color=Count)) + 
  geom_label_repel(data= common_words_brew %>% filter(Word %in% label), aes(label=Word)) +
  scale_colour_gradient(low='darkgreen',high='lightgreen') +
  labs(x = 'Average ABV', y = 'Average IBU', title = 'Average ABV & IBU by Word : Size & Color by Usage Count') +
  theme(legend.position='none')

```

```{r}
# We want to test if the mean/median ABV or IBU is significantly different between regions.

# Checking Assumptions

ggplot(new_df2, aes(x=ABV, y=Region)) + geom_boxplot()
ggplot(new_df2, aes(x=ABV)) + geom_histogram() + facet_wrap(~Region)
# While Regions have similar variances, I can't say that they are normally distributed. Although the CLT may allows us to bypass this assumption, I will stick with a non-parametric test to be safe.

ggplot(new_df2, aes(x=IBU, y=Region)) + geom_boxplot()
ggplot(new_df2, aes(x=IBU)) + geom_histogram() + facet_wrap(~Region)
# There is overwhelming evidence that the distributions are not normal and that regions do not have similar variance. Because of this, a non-parametric test is required.

# We are assuming independence both with-in and between groups.

# Non-parametric ANOVA (Kruskal Wallis Test)

kruskal.test(ABV~Region, new_df2)
# P-value < .05, so we say that there is evidence to suggest that at least one of the Region median ABVs is significantly different from the others.

kruskal.test(IBU~Region, new_df2)
# P-value > .05, so we say that there is not enough evidence to suggest that there is a difference in Region-specific ABV medians.

# To find which exact regions have differing median ABVs, we will use Rank Sum Tests. The function below calculates all combinations of the regions in our df and tests them using wilcox.test.

regions <- unique(as.character(new_df2$Region))
# Pull regions into a list

wilcoxon_tester <- function(myVar) {
  region_x <- character()
  region_y <- character()
  p_value <- numeric()
  
  for (i in 1:length(regions)) {
    
    for (j in 1:length(regions)) {
      
      if (j != i) {
        j_df <- new_df2 %>% filter(Region == regions[j]) %>% pull({{myVar}})
        i_df <- new_df2 %>% filter(Region == regions[i]) %>% pull({{myVar}})
        test <- wilcox.test(x=j_df, y=i_df, alternative='two.sided')
        
        if (test$p.value < .05) {
          region_x <- append(region_x, regions[i])
          region_y <- append(region_y, regions[j])
          p_value <- append(p_value, round(test$p.value, 4))
        }
      }
    }
  }
  wilcox_results <- data.frame('Region_x'=region_x, 'Region_y'=region_y, 'P_value'=p_value)
}

# Testing....will print out all tests that are significant.
wilcox_results <- wilcoxon_tester(ABV)

wilcox_results
```
For this question, we wanted to examine if the mean or median ABV / IBU was consistent throughtout regions.

While testing for ANOVA assumptions, it was clear that neither ABV or IBU was normally distributed or had similar variances between groups. Therefore, the assumptions did not pass. This meant that a non-parametric Kruskal Wallis test must be used instead of the ANOVA.

Using the Kruskal Wallis test, we examined if the median ABV / IBU was consistent throughout regions.

While examining ABV, the Kruskal Wallis test gave us a p-value of .0003. Because this is below our significance level of .05, the evidence suggests that at least one median ABV is significantly different than the others. To find out which regions had significant differences, a function was used that ran all region combinations through a Rank Sum Test (wilcox.test). Through this, we found that 8 different region combinations had a significant difference in their median ABVs (p-value < .05).

On the other hand, a Kruskal Wallis test on IBU gave us a p-value of .07. Because this is larger than .05, we can say that there is not enough evidence to suggest that there is a significant difference between any of the median IBUs of regions.
