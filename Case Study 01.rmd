---
title: "Case Study - 01"
author: "Eric Laigaie"
date: "10/5/2021"
output:
  html_document: default
  pdf_document: default
---

# Introduction:


### Package Load-In
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(tidyverse)
library(readr)
library(ggplot2)
library(choroplethr)
library(choroplethrMaps)
library(ggpubr)
library(class)
library(caret)
library(e1071)
library(multcomp)
library(DescTools)
```

### Beers and Breweries .csv Load-In
```{r}
# Load in Both Beers and Breweries datasets.
beers <- read_csv(url("https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/Beers.csv?token=AL7L5SUDXIN5C6OJ62LPMELBLTXMC"))
breweries <- read_csv(url("https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/Breweries.csv?token=AL7L5SSPZ7B6NQBXJIMILGDBLTXOM"))
```

## 1.   How many breweries are present in each state?
```{r State n, fig.height=5}
# Brewery Bar Chart
breweries %>%
  group_by(State) %>%
  summarize(n = n()) %>%
  ggplot(aes(x=n, y=reorder(State, n))) + 
  geom_bar(stat='identity', fill='darkseagreen4') + 
  labs(x='State', y='Number of Breweries', title='Number of Breweries by State') + 
  theme_minimal()

# Load in State Brewery Counts (This changes State name from "AL" -> "alabama")
states <- read_csv(url("https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/state_brewery_counts.csv"))

# Import geojson for state polygons
data(continental_us_states)

# Designate region & value
states$region <- tolower(states$State)
states$value <- states$n

# Create Choropleth
state_choropleth(states, 
                 num_colors=9,
                 zoom = continental_us_states) +
  scale_fill_brewer(palette="Greens") +
  labs(title = "Breweries by State",
       fill = "n") +
  guides(fill=guide_legend(title="N"))
```
Brewery count varies from state to state. Colorado, California, and Michigan lead the way with over 30 breweries while the bottom 33 states have less than 10 each. In an effort to find geographic patterns, a map was produced. After observation, no solid conclusion was made. One hypothesis is that coastline could increase the number of breweries, as both coasts, Texas, and the Great Lakes region holds the majority of breweries.


## 2. Merge beer data with the breweries data. Print the first 6 observations and the last six observations to check the merged file.  (RMD only, this does not need to be included in the presentation or the deck.)
```{r}
# Merge beers and breweries
breweries <- breweries %>% rename(Brewery_id=Brew_ID)
df <- merge(breweries, beers, by='Brewery_id')

df <- df %>% rename(Brewery = Name.x, Beer = Name.y)

# Print top & bottom 6
head(df, 6)
tail(df, 6)
```
After merging the datasets, we are left with a larger dataframe that not only holds information on each brewery, but the individuals beers inside of them.

## 3. Address the missing values in each column.
```{r}
# METHOD 3 - MEAN IMPUTATION BY STYLE GROUP

# Remove missing Styles
df <- df %>% filter(is.na(Style) == FALSE)

# Function takes in df and imputes mean ABV and IBU. Also creates Style_Group column
imputer <- function(mydf, myText) {
  temp <- {{mydf}}
  for(i in 7:8){
    temp[is.na(temp[,i]), i] <- mean(temp[,i], na.rm = TRUE)
  }
  temp$Style_Group <- {{myText}}
  output <- temp
}

# Creating new dfs for each style group
ale <- df %>% filter(grepl('Ale', df$Style) == TRUE) %>% filter(Style != 'English India Pale Ale (IPA)')
ipa <- df %>% filter(grepl('IPA', df$Style) == TRUE)
lager <- df %>% filter(grepl('Lager', df$Style) == TRUE)
stout <- df %>% filter(grepl('Stout', df$Style) == TRUE)
other <- df %>% filter(grepl('Stout', df$Style) == FALSE &
                grepl('Lager', df$Style) == FALSE &
                grepl('IPA', df$Style) == FALSE &
                grepl('IPA', df$Style) == FALSE &
                grepl('Ale', df$Style) == FALSE)

# Imputing mean ABV / IBU
ale <- imputer(ale, 'Ale')
ipa <- imputer(ipa, 'IPA')
lager <- imputer(lager, 'Lager')
stout <- imputer(stout, 'Stout')
other <- imputer(other, 'Other')

# Combine back into df
df <- rbind(ale, ipa, lager, stout, other)
df$Style_Group <- as.factor(df$Style_Group)
```
Because missing values are found in over 40% of the IBU column, it was important to avoid removing them outright. Therefore, the dataset was split into 5 groups based on beer style (Ale, IPA, Lager, Stout, and Other). Then, the missing values in each group were imputed with the group mean of that variable. Additionally, 5 missing Styles were found. Since this missing data is a small part of the full dataframe, these rows with missing values were removed.

## 4. Compute the median alcohol content and international bitterness unit for each state. Plot a bar chart to compare.
```{r med_abv, fig.height=5}
# Median ABV bar chart
df %>% group_by(State) %>% 
  summarise(med_abv = median(ABV)) %>%
  ggplot(aes(y=reorder(State, med_abv), x=med_abv)) + 
  geom_bar(stat='identity', fill='darkseagreen4') +
  labs(x='Median ABV', y='State', title='Median ABV by State')

# Median IBU bar chart
df %>% group_by(State) %>%
  summarise(med_ibu = median(IBU)) %>%
  ggplot(aes(y=reorder(State, med_ibu), x=med_ibu)) + 
  geom_bar(stat='identity', fill='darkseagreen4') +
  labs(x='Median IBU', y='State', title='Median IBU by State')
```
Both the median ABV and IBU charts do not show a lot of variation between states. 44/51 states have a median ABV between .05 and .06, and 40/51 states a median IBU between 30 and 50. Additionally, 25 states have an almost identical median IBU. This is most likely a product of these states having many missing IBU values that were imputed manually.


## 5. Which state has the maximum alcoholic (ABV) beer? Which state has the most bitter (IBU) beer?
```{r}
# Find max ABV & IBU
max_ABV = max(df$ABV)
max_IBU = max(df$IBU)
df %>% filter(ABV == max_ABV)
df %>% filter(IBU == max_IBU)

ABV_diff <- (max_ABV - mean(df$ABV)) / sd(df$ABV)
IBU_diff <- (max_IBU - mean(df$IBU)) / sd(df$IBU)

#ABV_diff ---> 5.09
#IBU_diff ---> 4.30
```
The max ABV is .128 (12.8%), which is 5.09 standard deviations above the mean of .06. The max IBU is 138, which is 4.30 standard deviations above the mean of 40.97.

## 6. Comment on the summary statistics and distribution of the ABV variable.
```{r}
# Quick summary of ABV - full dataset
summary(df$ABV)
sd(df$ABV)

# Make 4 charts
box_all <- ggplot(df, aes(x=ABV)) + geom_boxplot(fill='darkseagreen4') + 
  labs(x='ABV', title='ABV Boxplot')
hist_all <- ggplot(df, aes(x=ABV)) + geom_histogram(binwidth = .005, color='black', fill='darkseagreen4') +
  labs(x='ABV', title='ABV Histogram')

# Arrange 4 charts
ggarrange(box_all, hist_all, nrow=2, ncol=1)

# Check for normality
shapiro.test(df$ABV)
qqnorm(df$ABV, col='darkseagreen4', lwd=2)
```
Using both graphical and statistical methods, we concluded that the ABV distribution is skewed right. While there is a large numbers of beers centered around the mean of .06, there is a tail of more alcoholic beers towards the higher ABV side.

## 7. Is there an apparent relationship between the bitterness of the beer and its alcoholic content? Draw a scatter plot.  Make your best judgment of a relationship and EXPLAIN your answer.
```{r}
# Find correlation of IBU and ABV in df
cor_IBUABV = round(cor(df$IBU, df$ABV), 3)

# Make R label
label_x = paste(c("R =", cor_IBUABV), collapse = " ")

# IBU and ABV scatterplot (with R label)
ggplot(df, aes(x=IBU, ABV)) + 
  geom_point(size=2, color='darkseagreen3') + 
  geom_smooth(method='lm', color='darkseagreen4') +
  labs(x='International Bittering Units (IBU)', y='Alcohol by Volume (ABV)', title='IBU vs ABV') +
  geom_label(label=label_x, x=100, y=.025,label.padding = unit(0.55, "lines"), label.size = 0.35, color = "black", fill="#69b3a2")
```
There is a moderate positive correlation between ABV and IBU (R = .0557). As either variable increases/decreases, we can moderately expect the other to move in the same direction. 

## 8. Budweiser would also like to investigate the difference with respect to IBU and ABV between IPAs (India Pale Ales) and other types of Ale (any beer with “Ale” in its name other than IPA).  You decide to use KNN classification to investigate this relationship.  Provide statistical evidence one way or the other. You can of course assume your audience is comfortable with percentages … KNN is very easy to understand conceptually.
```{r}

# Create new data frame with only Ale and IPA Beers
# Next, create dummy variable for Ale vs IPA

df_ale = df %>% filter(grepl("Ale", df$Style) == TRUE) %>% filter(Style != 'English India Pale Ale (IPA)')
df_IPA = df %>% filter(grepl("IPA", df$Style) == TRUE)

df_IPA$ale_IPA = 'IPA'
df_ale$ale_IPA = 'Ale'

new_df = rbind(df_ale,df_IPA)

new_df$ale_IPA = as.factor(new_df$ale_IPA)

new_df <- df %>% filter(Style_Group %in% c('Ale', 'IPA'))

# Perform KNN model using ABV and IBU
# Hyper-tune to be completed on K value

# First, standardize IBU and ABV values

new_df$zABV = as.numeric(scale(new_df$ABV))
new_df$zIBU = as.numeric(scale(new_df$IBU))

# Create 70/30 Train/Test Split
len <- length(new_df$Style_Group)
trainInd = sample(seq(1,len,1), .7*len)
train = new_df[trainInd,]
test = new_df[-trainInd,]

# Initial run of KNN model with K=5 
classifications = knn(train[,c(7,8)],test[,c(7,8)],train$Style_Group,prob = TRUE, k = 5)
confusionMatrix(table(classifications,test$Style_Group))

# Loop to find best value for K with 70/30 train-test split

iterations = 100
numks = 100
splitPerc = .70

masterAcc = matrix(nrow = iterations, ncol = numks)
masterSen = matrix(nrow = iterations, ncol = numks)
masterSpe = matrix(nrow = iterations, ncol = numks)

for(j in 1:iterations)
{
  trainIndices = sample(1:dim(new_df)[1],round(splitPerc * dim(new_df)[1]))
  train = new_df[trainIndices,]
  test = new_df[-trainIndices,]
  for(i in 1:numks)
  {
    classifications = knn(train[,c(7,8)],test[,c(7,8)],train$Style_Group, prob = TRUE, k = i)
    table(classifications,test$Style_Group)
    CM = confusionMatrix(table(classifications,test$Style_Group))
    masterAcc[j,i] = CM$overall[1]
    masterSen[j,i] = CM$byClass[1]
    masterSpe[j,i] = CM$byClass[2]
  }
  
}

MeanAcc = colMeans(masterAcc)
MeanSen = colMeans(masterSen)
MeanSpe = colMeans(masterSpe)

plot(seq(1,numks,1),MeanAcc, type = "l")
which.max(MeanAcc)
max(MeanAcc)

plot(seq(1,numks,1),MeanSen, type = "l")
which.max(MeanSen)
max(MeanSen)

plot(seq(1,numks,1),MeanSpe, type = "l")
which.max(MeanSpe)
max(MeanSpe)

# K = 6 gives highest accuracy (81%)
# K = 10 gives highest sensitivity (87%)
# K = 62 gives highest specificity (73%)

# Run KNN model with K=6 for highest accuracy (per hyper tuning)
classifications = knn(train[,c(7,8)],test[,c(7,8)],train$Style_Group,prob = TRUE, k = 5)
confusionMatrix(table(classifications,test$Style_Group))

```
A KNN model using k=5 was found to produce the highest average accuracy at 91.3%. Therefore, an optimized KNN model could accurately predict >90% of beers as Ales or IPAs based solely on their ABV and IBU. This tells us that ABV and IBU is homogenous within the Ale or IPA style or sub-styles. Basically, if a beer with X ABV and Y IBU is an Ale, it is likely that the majority of the most similar beers are Ales as well.


## 8.5: Additional Modeling
In addition, while you have decided to use KNN to investigate this relationship (KNN is required) you may also feel free to supplement your response to this question with any other methods or techniques you have learned.  Creativity and alternative solutions are always encouraged.
```{r}
# Using Naive-Bayes to investigate relationship

# Set seed and create Train/Test Split 
set.seed(10)
trainIndices = sample(seq(1:length(new_df$Beer_ID)),round(.7*length(new_df$Beer_ID)))
train_nb = new_df[trainIndices,]
test_nb = new_df[-trainIndices,]

# Create model and print confusion matrix
model = naiveBayes(train_nb[,c(7,8)],train_nb$Style_Group,laplace = 1)
table(predict(model,test_nb[,c(7,8)]),test_nb$Style_Group)
CM = confusionMatrix(table(predict(model,test_nb[,c(7,8)]),test_nb$Style_Group))
CM


```

## 9. Knock their socks off!  Find one other useful inference from the data that you feel Budweiser may be able to find value in.  You must convince them why it is important and back up your conviction with appropriate statistical evidence.

### Load in Regions data and visualize.
```{r}
# Load in regions data
regions = read_csv(url('https://raw.githubusercontent.com/ericlaigaie/Case-Study-01/main/regions_v2.csv'))

# remove State Column and rename abbreviations to 'State' to merge
regions = subset(regions, select = -c(State))
names(regions)[2] = 'State'
df = merge(df,regions,"State")

# Prep data for choropleth
df$Region <- as.factor(df$Region)
regions <- regions %>% arrange(State)

# Designate region & value
choro_df <- data.frame('region' = states$State, 'value' = regions$Region)

# Create Choropleth
state_choropleth(choro_df, 
                 num_colors=7,
                 zoom = continental_us_states) +
  scale_fill_brewer(palette="Set1") +
  labs(title = "Regions",
       fill = "n") +
  guides(fill=guide_legend(title="Region"))
```
This is a map of our regions. The next code cell will use statistical tests to examine how ABV and IBU distributions differ between regions.

### Checking Assumptions of ANOVA
```{r}
# ABV
ggplot(df, aes(x=ABV, y=Region)) + geom_boxplot()
ggplot(df, aes(x=ABV)) + geom_histogram() + facet_wrap(~Region)

# IBU
ggplot(df, aes(x=IBU, y=Region)) + geom_boxplot()
ggplot(df, aes(x=IBU)) + geom_histogram() + facet_wrap(~Region)

# We are assuming independence both with-in and between groups.
```
ABV: While regions have similar variances, I can't say that they are normally distributed. Although the CLT may allows us to bypass this assumption, I will stick with a non-parametric test to be safe.

IBU: There is overwhelming evidence that the distributions are not normal and that regions do not have similar variance. Because of this, a non-parametric test is required.

### Kruskal Wallis Test
```{r}
kruskal.test(ABV~Region, df)
# P-value < .05, so we say that there is evidence to suggest that at least one of the Region median ABVs is significantly different from the others.

kruskal.test(IBU~Region, df)
# P-value > .05, so we say that there is not enough evidence to suggest that there is a difference in Region-specific ABV medians.
```
The Kruskal-Wallis test provides sufficient evidence to suggest that at least one region has a mean ABV that differs significantly from the others. Further testing down below (P-value = .00035).

The Kruskal-Wallis test suggests that there is not enough evidence to say that any region's mean IBU differs significantly from any others. No further testing needed (P-value = .0778).

### To find which exact regions have differing median ABVs, we will use Rank Sum Tests. The function below calculates all combinations of the regions in our df and tests them using wilcox.test.
```{r}
regions <- unique(as.character(df$Region))
# Pull regions into a list

wilcoxon_tester <- function(myVar) {
  region_x <- character()
  region_y <- character()
  p_value <- numeric()
  
  for (i in 1:length(regions)) {
    
    for (j in 1:length(regions)) {
      
      if (j != i) {
        j_df <- df %>% filter(Region == regions[j]) %>% pull({{myVar}})
        i_df <- df %>% filter(Region == regions[i]) %>% pull({{myVar}})
        test <- wilcox.test(x=j_df, y=i_df, alternative='two.sided')
        
        if (test$p.value < .05) {
          region_x <- append(region_x, regions[i])
          region_y <- append(region_y, regions[j])
          p_value <- append(p_value, round(test$p.value, 4))
        }
      }
    }
  }
  wilcox_results <- data.frame('Region_x'=region_x, 'Region_y'=region_y, 'P_value'=p_value)
}

# Testing....will print out all tests that are significant.
wilcox_results <- wilcoxon_tester(ABV)

wilcox_results
```
The Rank Sum Tests above tell us that 8 out of the 21 region combinations have significantly different mean ABVs. Displayed along the two regions is the p-value from the associated test.

# Conclusion:
